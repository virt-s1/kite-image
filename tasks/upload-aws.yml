---
- name: create $HOME/.aws folder to dave config and credentials
  file:
    path: ~/.aws
    state: directory
    mode: '0755'

- name: create credentials file
  template:
    src: aws_credentials.j2
    dest: ~/.aws/credentials
    mode: '0600'

- name: set default ami arch
  set_fact:
    ami_arch: "x86_64"
    of_disk: "/dev/xvdb"

- name: set ami arch to arm64 for aarch64 arch
  set_fact:
    ami_arch: "arm64"
    of_disk: "/dev/nvme1n1"
  when: arch == "aarch64"

- name: get all availability zones
  aws_az_info:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
  register: ec2_zones

- debug:
    var: ec2_zones

- name: create VPC
  ec2_vpc_net:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    name: "kite.imagebuild"
    cidr_block: "172.30.0.0/16"
    tags:
      kite: "kite.imagebuild"
  register: ec2_vpc

- debug:
    var: ec2_vpc

- name: add internet gateway for VPC
  ec2_vpc_igw:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    vpc_id: "{{ ec2_vpc.vpc.id }}"
    tags:
      kite: "kite.imagebuild"
  register: ec2_vpc_igw

- debug:
    var: ec2_vpc_igw

- name: "create subnet for VPC in {{ ec2_zones.availability_zones[0] }}"
  ec2_vpc_subnet:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    vpc_id: "{{ ec2_vpc.vpc.id }}"
    cidr: "172.30.30.0/24"
    az: "{{ ec2_zones.availability_zones[-2].zone_name }}"
    tags:
      kite: "kite.imagebuild"
  register: ec2_vpc_subnet

- debug:
    var: ec2_vpc_subnet

- name: add default route in route table for all vpc subnets
  ec2_vpc_route_table:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    vpc_id: "{{ ec2_vpc.vpc.id }}"
    subnets:
      - "{{ ec2_vpc_subnet.subnet.id }}"
    routes:
      - dest: 0.0.0.0/0
        gateway_id: "{{ ec2_vpc_igw.gateway_id }}"
    tags:
      kite: "kite.imagebuild"
  register: ec2_vpc_route_table

- debug:
    var: ec2_vpc_route_table

- name: get localhost public ip address
  uri:
    url: https://api.ipify.org?format=json
  register: result_ip

- debug:
    var: result_ip

- name: create security group
  ec2_group:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    vpc_id: "{{ ec2_vpc.vpc.id }}"
    name: "kite.imagebuild.{{ os }}.{{ arch }}"
    description: kite security group
    rules:
      - proto: all
        cidr_ip: "{{ result_ip.json.ip }}/32"
    tags:
      kite: "kite.imagebuild.{{ os }}.{{ arch }}"
  register: ec2_security_group

- debug:
    var: ec2_security_group

- name: delete keypare first if exist
  ec2_key:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    name: "kite.imagebuild.{{ os }}.{{ arch }}"
    state: absent

- name: generate and add ssh keypair
  ec2_key:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    name: "kite.imagebuild.{{ os }}.{{ arch }}"
    key_material: "{{ lookup('file', 'key/identity.pub')  }}"

- name: create ec2 instance
  ec2:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    assign_public_ip: yes
    group_id: "{{ ec2_security_group.group_id }}"
    image: "{{ fedora_ami[arch] }}"
    instance_type: "{{ builder_instance_type[arch] }}"
    vpc_subnet_id: "{{ ec2_vpc_subnet.subnet.id }}"
    key_name: "kite.imagebuild.{{ os }}.{{ arch }}"
    zone: "{{ ec2_zones.availability_zones[-2].zone_name }}"
    instance_initiated_shutdown_behavior: terminate # required by spot instances
    spot_price: "0.1"
    spot_type: one-time
    state: present
    volumes:
      - device_name: /dev/sda1
        volume_size: 20
        volume_type: gp2
        delete_on_termination: true
    user_data: |
      #cloud-config
      packages:
        - python3
        - python3-dnf
        - qemu-img
    instance_tags:
      kite: "kite.imagebuild.{{ os }}.{{ arch }}"
    wait: yes
  register: ec2_instance

- debug:
    var: ec2_instance

- name: waits until ssh is reachable
  wait_for:
    host: "{{ ec2_instance.instances[0].public_ip }}"
    port: 22
    search_regex: OpenSSH
    delay: 10

- name: add instance to dder group
  add_host:
    name: "{{ ec2_instance.instances[0].public_ip }}"
    group: dder

- name: keep private key permission to 600
  file:
    path: "{{ playbook_dir }}/key/identity"
    mode: "0600"

- name: ensure cloud-init has finished
  raw: test -f /var/lib/cloud/instance/boot-finished
  retries: 60
  register: cloud_init_check
  changed_when: false
  until: cloud_init_check is success
  delegate_to: "{{ ec2_instance.instances[0].public_ip }}"

- name: attach a new volume for dd
  ec2_vol:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    instance: "{{ ec2_instance.instance_ids[0] }}"
    name: image_snapshot
    device_name: /dev/sdb
    zone: "{{ ec2_zones.availability_zones[-2].zone_name }}"
    volume_size: "12"
    volume_type: gp2
    tags:
      kite: "kite.imagebuild.{{ os }}.{{ arch }}"
  register: ec2_volume

- debug:
    var: ec2_volume

- name: upload qcow2 image to instance
  copy:
    src: "{{ playbook_dir }}/{{ os }}_{{ arch }}.qcow2"
    dest: /home/fedora
  delegate_to: "{{ ec2_instance.instances[0].public_ip }}"

- name: convert qcow2 to raw for dd
  command: "qemu-img convert -O raw /home/fedora/{{ os }}_{{ arch }}.qcow2 /home/fedora/{{ os }}_{{ arch }}.raw"
  become: yes
  delegate_to: "{{ ec2_instance.instances[0].public_ip }}"

- name: dd raw file to the 2nd volume
  command: "dd if=/home/fedora/{{ os }}_{{ arch }}.raw of={{ of_disk }}"
  become: yes
  delegate_to: "{{ ec2_instance.instances[0].public_ip }}"

- name: check disk status
  command: fdisk -l
  become: yes
  register: result_fdisk
  delegate_to: "{{ ec2_instance.instances[0].public_ip }}"

- debug:
    var: result_fdisk

- name: terminate instance
  ec2:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    instance_ids: "{{ ec2_instance.instance_ids[0] }}"
    state: absent
    wait: yes

- name: make snapshot of attached volume
  ec2_snapshot:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    volume_id: "{{ ec2_volume.volume_id }}"
    description: "snapshot {{ os }} image taken on {{ ansible_date_time.iso8601  }}"
    snapshot_tags:
      kite: "kite.imagebuild.{{ os }}.{{ arch }}"
  register: ec2_snapshot

- debug:
    var: ec2_snapshot

- name: get old ami info
  ec2_ami_info:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    filters:
      "tag:kite": "kite.imagebuild.{{ os }}.{{ arch }}"
  register: result_ami

- debug:
    var: result_ami

- name: deregister old amis
  ec2_ami:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    image_id: "{{ item }}"
    delete_snapshot: True
    state: absent
  loop: "{{ result_ami.images | map(attribute='image_id') | list }}"

- name: AMI registration from EBS Snapshot
  ec2_ami:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    name: "kite.imagebuild.{{ os }}.{{ arch }}"
    description: "{{ os }} {{ arch }} ami built on {{ ansible_date_time.iso8601  }}"
    state: present
    architecture: "{{ ami_arch }}"
    virtualization_type: hvm
    root_device_name: /dev/xvda
    device_mapping:
      - device_name: /dev/xvda
        volume_size: 12
        snapshot_id: "{{ ec2_snapshot.snapshot_id }}"
        delete_on_termination: true
        volume_type: gp2
    enhanced_networking: yes
    sriov_net_support: simple
    tags:
      kite: "kite.imagebuild.{{ os }}.{{ arch }}"
  register: ec2_ami

- debug:
    var: ec2_ami

- name: save ami id into ssm parameter store
  aws_ssm_parameter_store:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    name: "kite_imagebuild_{{ os }}_{{ arch }}"
    description: "Save latest {{ os }} {{ arch }} image id"
    value: "{{ ec2_ami.image_id }}"

- name: delete the 2nd EBS volume
  ec2_vol:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    id: "{{ ec2_volume.volume_id }}"
    state: absent

- name: delete keypair
  ec2_key:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    name: "kite.imagebuild.{{ os }}.{{ arch }}"
    state: absent

- name: delete security group
  ec2_group:
    profile: "{{ aws_profile }}"
    region: "{{ aws_region }}"
    group_id: "{{ ec2_security_group.group_id }}"
    state: absent
